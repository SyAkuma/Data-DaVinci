{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LOGISTIC REGRESSION\n",
    "In this assignment we try to find a relation between average weight of granules and total surface area to see if a material is viable as a catalyst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing libraries here\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature mapping\n",
    "Sometimes, we are given an inadequate number of features for which training the dataset becomes difficult.  \n",
    "Hence we create new features of by taking polynomial products of existing ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generates new features by taking products of various powers of the input features (x and y)\n",
    "# This expands the feature space with polynomial combinations (up to degree 4) of x and y\n",
    "\n",
    "### TODO 1\n",
    "def feature_map(points):\n",
    "    points = np.array(points)\n",
    "    x, y = points[:, 0], points[:, 1]\n",
    "    \n",
    "    features = []\n",
    "    # YOUR CODE HERE\n",
    "    # Append different terms such as x, y^2, x^2y and so on upto degree 4 terms\n",
    "    features.append(x)\n",
    "    features.append(y)\n",
    "    features.append(x**2)\n",
    "    features.append(y**2)\n",
    "    features.append(x*y)\n",
    "    features.append(x**3)\n",
    "    features.append(y**3)\n",
    "    features.append(x*(y**2))\n",
    "    features.append((x**2)*y)\n",
    "    features.append(x**4)\n",
    "    features.append(y**4)\n",
    "    features.append((x**3)*y)\n",
    "    features.append((y**3)*x)\n",
    "    features.append((x**2)*(y**2))\n",
    "    \n",
    "    return np.column_stack(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the class for Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our class in which we define all functions. (This way our functions are contained)\n",
    "class LogisticRegression:\n",
    "\n",
    "    # Constructor (weights and bias are member variables)\n",
    "    # Can be accessed via self.weights and self.bias\n",
    "    def __init__(self) -> None:\n",
    "        self.weights : np.ndarray | None = None\n",
    "        self.bias : float | None = None\n",
    "\n",
    "\n",
    "\n",
    "    # Sigmoid function\n",
    "    ### TODO 2\n",
    "    def __sigmoid(self, z: np.ndarray) -> np.ndarray:\n",
    "        # YOUR CODE HERE\n",
    "        return (1/(1+np.exp(-z)))\n",
    "\n",
    "\n",
    "    \n",
    "    # Returns probabilities of being true\n",
    "    ### TODO 3\n",
    "    def predict_probability(self, X: np.ndarray) -> np.ndarray:\n",
    "        # YOUR CODE HERE\n",
    "        Y_pred=np.dot(self.weights,X)+self.bias\n",
    "        return self.__sigmoid(Y_pred)\n",
    "\n",
    "\n",
    "    # Returns true/false (based on the probabilities)\n",
    "    ### TODO 4\n",
    "    def predict(self, X: np.ndarray) -> np.ndarray:\n",
    "        # YOUR CODE HERE\n",
    "        probabilities=self.predict_probability(X)\n",
    "        predictions= probabilities>=0.5\n",
    "        return predictions\n",
    "\n",
    "\n",
    "    \n",
    "    # Returns loss or cost, change in weights (dw), change in bias (db) with regularization\n",
    "    ### TODO 5 \n",
    "    def __loss(self, X: np.ndarray, y: np.ndarray, lambda_reg: float = 0) -> tuple:\n",
    "        # Add code here\n",
    "        predictions=self.predict_probability(X)\n",
    "        loss = - (1/X.shape[0]) * (np.dot(y, np.log(predictions + 1e-12)) + np.dot(1 - y, np.log(1 - predictions + 1e-12)))\n",
    "        dw = (1/X.shape[0])*()\n",
    "        db = None\n",
    "\n",
    "        # L2 Regularization\n",
    "        loss += None ## Change this line\n",
    "        dw += None ## Change this line\n",
    "\n",
    "        return loss, dw, db\n",
    "    \n",
    "\n",
    "\n",
    "    # Adjusts the weights and bias to get the minimum loss\n",
    "    ### TODO 6\n",
    "    def fit(self, X: np.ndarray, y: np.ndarray, epochs: int = 500,\n",
    "            learning_rate: float = 0.01, threshold: float = 0.0001, \n",
    "            lambda_reg: float = 1) -> None:\n",
    "        # Randomise initial weights and bias\n",
    "        # YOUR CODE HERE\n",
    "\n",
    "        # Gradient descent\n",
    "        for _ in range(epochs):\n",
    "            # Logic for gradient descent\n",
    "            # YOUR CODE HERE\n",
    "            pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing data\n",
    "df = pd.read_csv('logistic_data.csv')\n",
    "data = df.to_numpy()\n",
    "X = data[:, :2]\n",
    "y = data[:, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating train-test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# z-score standardization\n",
    "### TODO 7\n",
    "def z_score(X: np.ndarray) -> tuple:\n",
    "    # Add code here\n",
    "    x_mean = np.mean(x)\n",
    "    x_std = np.std(x,axis=0)\n",
    "    x = (x-x_mean)/x_std\n",
    "    return x, x_mean, x_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalizing the data (we use the same constants to maintain consistency)\n",
    "X_train, x_mean, x_std = z_score(X_train)\n",
    "X_test = (X_test - x_mean) / x_std\n",
    "x_train = feature_map(X_train)\n",
    "x_test = feature_map(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing how the boundary curve looks like\n",
    "def plot_decision_boundary(X_original, y, model, resolution=500):\n",
    "    # Set up the grid for the decision boundary\n",
    "    x_min, x_max = X_original[:, 0].min() - 1, X_original[:, 0].max() + 1\n",
    "    y_min, y_max = X_original[:, 1].min() - 1, X_original[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.linspace(x_min, x_max, resolution),\n",
    "                         np.linspace(y_min, y_max, resolution))\n",
    "    \n",
    "    # Flatten the grid points and map to expanded features\n",
    "    grid_original = np.c_[xx.ravel(), yy.ravel()]\n",
    "    grid_expanded = feature_map(grid_original)\n",
    "    \n",
    "    # Predict the grid values for decision boundary\n",
    "    Z = model.predict(grid_expanded)\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    \n",
    "    # Plot the data points\n",
    "    true_points = X_original[y == 1]\n",
    "    false_points = X_original[y == 0]\n",
    "    plt.scatter(true_points[:, 0], true_points[:, 1], label=\"True\", c=\"blue\", marker=\"o\", s=20)\n",
    "    plt.scatter(false_points[:, 0], false_points[:, 1], label=\"False\", c=\"red\", marker=\"x\", s=20)\n",
    "\n",
    "    # Plot the decision boundary\n",
    "    plt.contour(xx, yy, Z, levels=[0.5], colors=\"black\", linewidths=2)\n",
    "    \n",
    "    # Labeling and title\n",
    "    plt.xlabel(\"Feature 1\")\n",
    "    plt.ylabel(\"Feature 2\")\n",
    "    plt.title(\"Decision Boundary and Data Points\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We plot the decision boundary that the model predicts. This can be used to check for overfitting.  \n",
    "If the boundary starts looking like an ameoba trying to fit every point, then it is a sign of overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking accuracy of test model\n",
    "model = LogisticRegression()\n",
    "model.fit(x_train, y_train, epochs=, learning_rate=, threshold=, lambda_reg=)\n",
    "y_pred = model.predict(x_test)\n",
    "\n",
    "accuracy = np.mean(y_pred == y_test) * 100\n",
    "print(f\"Your model has an accuracy of {accuracy}%\")\n",
    "\n",
    "# Plotting the contour and checking for overfitting (try changing the degree in the feature_map function to 10 instead)\n",
    "plot_decision_boundary(((X - x_mean) / x_std), y, model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
